{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + PostgreSQL Vector Database ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ PostgreSQL + pgvectorë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì„ë² ë”©ì„ ì €ì¥í•˜ê³ ,  \n",
    "ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í†µí•´ ë” ì •í™•í•œ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Vector Database\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# Ollama\n",
    "import ollama\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë° ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'vector_qa',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres'\n",
    "}\n",
    "\n",
    "# ì„ë² ë”© ì„¤ì •\n",
    "EMBEDDING_MODEL = 'nomic-embed-text'  # Ollama ì„ë² ë”© ëª¨ë¸\n",
    "EMBEDDING_DIMENSION = 768  # ì„ë² ë”© ì°¨ì›ìˆ˜\n",
    "CHUNK_SIZE = 1000  # í…ìŠ¤íŠ¸ ì²­í¬ í¬ê¸°\n",
    "\n",
    "print(\"ì„¤ì • ì™„ë£Œ âš™ï¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDatabase:\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.conn = None\n",
    "        self.embedding_model = EMBEDDING_MODEL\n",
    "        self.embedding_dim = EMBEDDING_DIMENSION\n",
    "    \n",
    "    def connect(self) -> bool:\n",
    "        \"\"\"ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°\"\"\"\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(**self.config)\n",
    "            self.conn.autocommit = True\n",
    "            register_vector(self.conn)\n",
    "            print(\"âœ… PostgreSQL ì—°ê²° ì„±ê³µ\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "            print(\"\\nğŸ’¡ PostgreSQL ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "            print(\"   brew services start postgresql@14\")\n",
    "            print(\"   ë˜ëŠ” Docker: docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:14\")\n",
    "            return False\n",
    "    \n",
    "    def create_database_if_not_exists(self):\n",
    "        \"\"\"ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±\"\"\"\n",
    "        try:\n",
    "            # postgres ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•˜ì—¬ vector_qa ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n",
    "            temp_config = self.config.copy()\n",
    "            temp_config['database'] = 'postgres'\n",
    "            \n",
    "            temp_conn = psycopg2.connect(**temp_config)\n",
    "            temp_conn.autocommit = True\n",
    "            \n",
    "            with temp_conn.cursor() as cur:\n",
    "                # ë°ì´í„°ë² ì´ìŠ¤ ì¡´ì¬ í™•ì¸\n",
    "                cur.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (self.config['database'],))\n",
    "                if not cur.fetchone():\n",
    "                    cur.execute(f\"CREATE DATABASE {self.config['database']}\")\n",
    "                    print(f\"âœ… ë°ì´í„°ë² ì´ìŠ¤ '{self.config['database']}' ìƒì„± ì™„ë£Œ\")\n",
    "                else:\n",
    "                    print(f\"â„¹ï¸  ë°ì´í„°ë² ì´ìŠ¤ '{self.config['database']}' ì´ë¯¸ ì¡´ì¬\")\n",
    "            \n",
    "            temp_conn.close()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    def setup_tables(self):\n",
    "        \"\"\"í•„ìš”í•œ í…Œì´ë¸” ë° í™•ì¥ ì„¤ì •\"\"\"\n",
    "        if not self.conn:\n",
    "            print(\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                # pgvector í™•ì¥ ì„¤ì¹˜\n",
    "                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "                print(\"âœ… pgvector í™•ì¥ ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "                \n",
    "                # ë¬¸ì„œ í…Œì´ë¸” ìƒì„±\n",
    "                cur.execute(f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS documents (\n",
    "                        id SERIAL PRIMARY KEY,\n",
    "                        filename VARCHAR(255) NOT NULL,\n",
    "                        content TEXT NOT NULL,\n",
    "                        chunk_index INTEGER NOT NULL,\n",
    "                        chunk_text TEXT NOT NULL,\n",
    "                        embedding vector({self.embedding_dim}),\n",
    "                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                        metadata JSONB\n",
    "                    )\n",
    "                \"\"\")\n",
    "                \n",
    "                # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±\n",
    "                cur.execute(\"\"\"\n",
    "                    CREATE INDEX IF NOT EXISTS documents_embedding_idx \n",
    "                    ON documents USING ivfflat (embedding vector_cosine_ops) \n",
    "                    WITH (lists = 100)\n",
    "                \"\"\")\n",
    "                \n",
    "                print(\"âœ… í…Œì´ë¸” ë° ì¸ë±ìŠ¤ ì„¤ì • ì™„ë£Œ\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…Œì´ë¸” ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"ğŸ”’ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ\")\n",
    "\n",
    "# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”\n",
    "vector_db = VectorDatabase(DB_CONFIG)\n",
    "print(\"VectorDatabase í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ ğŸ—„ï¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ì—°ê²° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (í•„ìš”í•œ ê²½ìš°)\n",
    "vector_db.create_database_if_not_exists()\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°\n",
    "if vector_db.connect():\n",
    "    # í…Œì´ë¸” ì„¤ì •\n",
    "    vector_db.setup_tables()\n",
    "else:\n",
    "    print(\"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. PostgreSQL ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì„ë² ë”© ìƒì„± ë° ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str, model: str = EMBEDDING_MODEL) -> List[float]:\n",
    "    \"\"\"\n",
    "    Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸\n",
    "        model (str): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: ì„ë² ë”© ë²¡í„°\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = ollama.embeddings(model=model, prompt=text)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        print(f\"ğŸ’¡ '{model}' ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: ollama pull {model}\")\n",
    "        return None\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \n",
    "    \n",
    "    Args:\n",
    "        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸\n",
    "        chunk_size (int): ì²­í¬ í¬ê¸°\n",
    "        overlap (int): ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        if end >= len(text):\n",
    "            chunks.append(text[start:])\n",
    "            break\n",
    "        \n",
    "        # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°\n",
    "        while end > start and text[end] not in ' \\n\\t.!?;':\n",
    "            end -= 1\n",
    "        \n",
    "        if end == start:\n",
    "            end = start + chunk_size\n",
    "        \n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_and_embed_documents(doc_folder: str = 'doc') -> bool:\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ ë¡œë”©í•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n",
    "    \n",
    "    Args:\n",
    "        doc_folder (str): ë¬¸ì„œ í´ë” ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        bool: ì„±ê³µ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        return False\n",
    "    \n",
    "    doc_path = Path(doc_folder)\n",
    "    if not doc_path.exists():\n",
    "        print(f\"âŒ '{doc_folder}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\")\n",
    "        return False\n",
    "    \n",
    "    md_files = list(doc_path.glob('*.md'))\n",
    "    if not md_files:\n",
    "        print(f\"âŒ '{doc_folder}' í´ë”ì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸ“š {len(md_files)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    try:\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì„ íƒì‚¬í•­)\n",
    "            # cur.execute(\"DELETE FROM documents\")\n",
    "            \n",
    "            total_chunks = 0\n",
    "            \n",
    "            for md_file in md_files:\n",
    "                print(f\"\\nğŸ“„ ì²˜ë¦¬ ì¤‘: {md_file.name}\")\n",
    "                \n",
    "                try:\n",
    "                    with open(md_file, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # í…ìŠ¤íŠ¸ ì²­í‚¹\n",
    "                    chunks = chunk_text(content)\n",
    "                    print(f\"  ğŸ“ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• \")\n",
    "                    \n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        if len(chunk.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ìŠ¤í‚µ\n",
    "                            continue\n",
    "                        \n",
    "                        # ì„ë² ë”© ìƒì„±\n",
    "                        embedding = generate_embedding(chunk)\n",
    "                        if embedding is None:\n",
    "                            continue\n",
    "                        \n",
    "                        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n",
    "                        metadata = {\n",
    "                            'file_size': len(content),\n",
    "                            'chunk_size': len(chunk)\n",
    "                        }\n",
    "                        \n",
    "                        cur.execute(\"\"\"\n",
    "                            INSERT INTO documents \n",
    "                            (filename, content, chunk_index, chunk_text, embedding, metadata)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                        \"\"\", (\n",
    "                            md_file.name,\n",
    "                            content,\n",
    "                            i,\n",
    "                            chunk,\n",
    "                            embedding,\n",
    "                            json.dumps(metadata)\n",
    "                        ))\n",
    "                        \n",
    "                        total_chunks += 1\n",
    "                        \n",
    "                        if (i + 1) % 5 == 0:\n",
    "                            print(f\"    âš¡ {i + 1}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ {md_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"\\nâœ… ì´ {total_chunks}ê°œ ì²­í¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë¬¸ì„œ ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ ğŸ“š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë¬¸ì„œ ë¡œë”© ë° ì„ë² ë”© ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸\n",
    "try:\n",
    "    test_embedding = generate_embedding(\"í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸\")\n",
    "    if test_embedding:\n",
    "        print(f\"âœ… ì„ë² ë”© ëª¨ë¸ '{EMBEDDING_MODEL}' ì‚¬ìš© ê°€ëŠ¥\")\n",
    "        print(f\"ğŸ“ ì„ë² ë”© ì°¨ì›: {len(test_embedding)}\")\n",
    "        \n",
    "        # ì„ë² ë”© ì°¨ì› ì—…ë°ì´íŠ¸ (ì‹¤ì œ ëª¨ë¸ì˜ ì°¨ì›ê³¼ ë§ì¶¤)\n",
    "        if len(test_embedding) != EMBEDDING_DIMENSION:\n",
    "            EMBEDDING_DIMENSION = len(test_embedding)\n",
    "            vector_db.embedding_dim = EMBEDDING_DIMENSION\n",
    "            print(f\"ğŸ”§ ì„ë² ë”© ì°¨ì›ì„ {EMBEDDING_DIMENSION}ìœ¼ë¡œ ì—…ë°ì´íŠ¸\")\n",
    "    else:\n",
    "        print(f\"âŒ ì„ë² ë”© ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: ollama pull {EMBEDDING_MODEL}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ë¡œë”© ë° ì„ë² ë”© ìƒì„± ì‹¤í–‰\n",
    "if vector_db.conn:\n",
    "    success = load_and_embed_documents('doc')\n",
    "    if success:\n",
    "        # ì €ì¥ëœ ë¬¸ì„œ í†µê³„ í™•ì¸\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM documents\")\n",
    "            total_chunks = cur.fetchone()[0]\n",
    "            \n",
    "            cur.execute(\"SELECT COUNT(DISTINCT filename) FROM documents\")\n",
    "            total_files = cur.fetchone()[0]\n",
    "            \n",
    "            print(f\"\\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©:\")\n",
    "            print(f\"  ğŸ“ íŒŒì¼ ìˆ˜: {total_files}ê°œ\")\n",
    "            print(f\"  ğŸ“„ ì²­í¬ ìˆ˜: {total_chunks}ê°œ\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query: str, limit: int = 5, similarity_threshold: float = 0.7) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ë²¡í„° ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰\n",
    "    \n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "        limit (int): ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜\n",
    "        similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        return []\n",
    "    \n",
    "    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "    query_embedding = generate_embedding(query)\n",
    "    if not query_embedding:\n",
    "        print(\"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT \n",
    "                    id,\n",
    "                    filename,\n",
    "                    chunk_index,\n",
    "                    chunk_text,\n",
    "                    metadata,\n",
    "                    1 - (embedding <=> %s) AS similarity\n",
    "                FROM documents\n",
    "                WHERE 1 - (embedding <=> %s) > %s\n",
    "                ORDER BY embedding <=> %s\n",
    "                LIMIT %s\n",
    "            \"\"\", (query_embedding, query_embedding, similarity_threshold, query_embedding, limit))\n",
    "            \n",
    "            results = []\n",
    "            for row in cur.fetchall():\n",
    "                results.append({\n",
    "                    'id': row[0],\n",
    "                    'filename': row[1],\n",
    "                    'chunk_index': row[2],\n",
    "                    'chunk_text': row[3],\n",
    "                    'metadata': json.loads(row[4]) if row[4] else {},\n",
    "                    'similarity': float(row[5])\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "def vector_qa(question: str, model_name: str = 'llama3.1', max_context_length: int = 4000) -> str:\n",
    "    \"\"\"\n",
    "    ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ\n",
    "    \n",
    "    Args:\n",
    "        question (str): ì§ˆë¬¸\n",
    "        model_name (str): ì‚¬ìš©í•  Ollama ëª¨ë¸\n",
    "        max_context_length (int): ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        str: ë‹µë³€\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” ì§ˆë¬¸ ë¶„ì„ ì¤‘: {question}\")\n",
    "    \n",
    "    # ê´€ë ¨ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰\n",
    "    similar_chunks = search_similar_chunks(question, limit=10, similarity_threshold=0.3)\n",
    "    \n",
    "    if not similar_chunks:\n",
    "        return \"âŒ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.\"\n",
    "    \n",
    "    print(f\"ğŸ“š {len(similar_chunks)}ê°œ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ ë°œê²¬\")\n",
    "    \n",
    "    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ê¸¸ì´ ì œí•œ)\n",
    "    context_parts = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for chunk in similar_chunks:\n",
    "        chunk_text = f\"=== {chunk['filename']} (ìœ ì‚¬ë„: {chunk['similarity']:.3f}) ===\\n{chunk['chunk_text']}\\n\"\n",
    "        \n",
    "        if current_length + len(chunk_text) > max_context_length:\n",
    "            break\n",
    "        \n",
    "        context_parts.append(chunk_text)\n",
    "        current_length += len(chunk_text)\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    prompt = f\"\"\"ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "ê´€ë ¨ ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€ ê·œì¹™:\n",
    "1. ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”\n",
    "2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”\n",
    "3. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì½”ë“œë¥¼ í¬í•¨í•˜ì„¸ìš”\n",
    "4. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œëª…ì„ ì–¸ê¸‰í•˜ì„¸ìš”\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ¤– {model_name} ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        response = ollama.chat(model=model_name, messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        answer = response['message']['content']\n",
    "        \n",
    "        # ì°¸ê³  ë¬¸ì„œ ì •ë³´ ì¶”ê°€\n",
    "        referenced_files = set(chunk['filename'] for chunk in similar_chunks[:5])\n",
    "        references = f\"\\n\\nğŸ“‹ **ì°¸ê³  ë¬¸ì„œ:** {', '.join(referenced_files)}\"\n",
    "        \n",
    "        return answer + references\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}\"\n",
    "\n",
    "def show_similar_chunks(question: str, limit: int = 5):\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ë“¤ì„ í‘œì‹œ\n",
    "    \n",
    "    Args:\n",
    "        question (str): ê²€ìƒ‰ ì§ˆë¬¸\n",
    "        limit (int): í‘œì‹œí•  ê²°ê³¼ ìˆ˜\n",
    "    \"\"\"\n",
    "    chunks = search_similar_chunks(question, limit=limit, similarity_threshold=0.2)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"âŒ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ” '{question}'ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ë“¤:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n{i}. ğŸ“„ {chunk['filename']} (ì²­í¬ {chunk['chunk_index']})\")\n",
    "        print(f\"   ğŸ¯ ìœ ì‚¬ë„: {chunk['similarity']:.3f}\")\n",
    "        print(f\"   ğŸ“ ë‚´ìš©: {chunk['chunk_text'][:200]}{'...' if len(chunk['chunk_text']) > 200 else ''}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "print(\"ë²¡í„° ê²€ìƒ‰ ë° QA í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ ğŸ”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "if vector_db.conn:\n",
    "    test_query = \"Network API ì‚¬ìš©ë²•\"\n",
    "    print(f\"ğŸ§ª í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query}'\")\n",
    "    \n",
    "    # ìœ ì‚¬ ë¬¸ì„œ ì²­í¬ í‘œì‹œ\n",
    "    show_similar_chunks(test_query, limit=3)\n",
    "else:\n",
    "    print(\"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë²¡í„° ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì‹œ ì§ˆì˜ì‘ë‹µ\n",
    "sample_questions = [\n",
    "    \"íŒŒì¼ ì—…ë¡œë“œëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?\",\n",
    "    \"ì¹´ë©”ë¼ ì´¬ì˜ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€?\",\n",
    "    \"JWT í† í° ê´€ë¦¬ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\"\n",
    "]\n",
    "\n",
    "if vector_db.conn:\n",
    "    print(\"ğŸ¤– ë²¡í„° ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì˜ˆì‹œ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, question in enumerate(sample_questions, 1):\n",
    "        print(f\"\\n{i}. â“ ì§ˆë¬¸: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        answer = vector_qa(question)\n",
    "        print(f\"ğŸ’¬ ë‹µë³€:\\n{answer}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if i < len(sample_questions):\n",
    "            input(\"\\nâ³ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°€ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ëŒ€í™”í˜• ë²¡í„° ì§ˆì˜ì‘ë‹µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_vector_qa():\n",
    "    \"\"\"\n",
    "    ëŒ€í™”í˜• ë²¡í„° ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸš€ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"íŠ¹ì§•:\")\n",
    "    print(\"  ğŸ” ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰\")\n",
    "    print(\"  ğŸ“š PostgreSQL + pgvector í™œìš©\")\n",
    "    print(\"  ğŸ¯ ë¬¸ì„œ ì²­í‚¹ ë° ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰\")\n",
    "    print()\n",
    "    print(\"ì‚¬ìš©ë²•:\")\n",
    "    print(\"  - ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤\")\n",
    "    print(\"  - 'quit' ë˜ëŠ” 'exit'ë¡œ ì¢…ë£Œ\")\n",
    "    print(\"  - '!search [ì§ˆë¬¸]'ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ì„œë§Œ ê²€ìƒ‰\")\n",
    "    print(\"  - '!stats'ë¡œ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ í™•ì¸\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nğŸ¤– ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:\n",
    "            print(\"ğŸ‘‹ ë²¡í„° ì§ˆì˜ì‘ë‹µì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            break\n",
    "        \n",
    "        if question.startswith('!search '):\n",
    "            search_query = question[8:].strip()\n",
    "            show_similar_chunks(search_query, limit=5)\n",
    "            continue\n",
    "        \n",
    "        if question == '!stats':\n",
    "            try:\n",
    "                with vector_db.conn.cursor() as cur:\n",
    "                    cur.execute(\"SELECT COUNT(*) FROM documents\")\n",
    "                    total_chunks = cur.fetchone()[0]\n",
    "                    \n",
    "                    cur.execute(\"SELECT COUNT(DISTINCT filename) FROM documents\")\n",
    "                    total_files = cur.fetchone()[0]\n",
    "                    \n",
    "                    cur.execute(\"SELECT filename, COUNT(*) FROM documents GROUP BY filename\")\n",
    "                    file_stats = cur.fetchall()\n",
    "                    \n",
    "                    print(f\"\\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:\")\n",
    "                    print(f\"  ğŸ“ ì´ íŒŒì¼ ìˆ˜: {total_files}ê°œ\")\n",
    "                    print(f\"  ğŸ“„ ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ\")\n",
    "                    print(f\"  ğŸ“‹ íŒŒì¼ë³„ ì²­í¬ ìˆ˜:\")\n",
    "                    for filename, count in file_stats:\n",
    "                        print(f\"    - {filename}: {count}ê°œ\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if not question:\n",
    "            print(\"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        answer = vector_qa(question)\n",
    "        print(\"\\nğŸ’¬ ë‹µë³€:\")\n",
    "        print(answer)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "# ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ ì‹œì‘ (í•„ìš”ì‹œ ì‹¤í–‰)\n",
    "# interactive_vector_qa()\n",
    "\n",
    "print(\"ëŒ€í™”í˜• ë²¡í„° QA í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ ğŸ’¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì»¤ìŠ¤í…€ ë²¡í„° ì§ˆì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ì ‘ ì§ˆë¬¸ ì…ë ¥í•˜ì—¬ í…ŒìŠ¤íŠ¸\n",
    "custom_question = \"bizMOBì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”\"\n",
    "\n",
    "if custom_question.strip() and vector_db.conn:\n",
    "    print(f\"â“ ì‚¬ìš©ì ì§ˆë¬¸: {custom_question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ë²¡í„° ê¸°ë°˜ ë‹µë³€ ìƒì„±\n",
    "    answer = vector_qa(custom_question)\n",
    "    print(f\"\\nğŸ’¬ ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€:\")\n",
    "    display(Markdown(answer))\n",
    "    \n",
    "    print(\"\\nğŸ” ê´€ë ¨ ë¬¸ì„œ ì²­í¬ ë¶„ì„:\")\n",
    "    show_similar_chunks(custom_question, limit=3)\n",
    "elif not custom_question.strip():\n",
    "    print(\"ìœ„ì˜ custom_question ë³€ìˆ˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„ ë„êµ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search_methods(question: str):\n",
    "    \"\"\"\n",
    "    ë²¡í„° ê²€ìƒ‰ê³¼ ê¸°ì¡´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë¹„êµ\n",
    "    \n",
    "    Args:\n",
    "        question (str): ë¹„êµí•  ì§ˆë¬¸\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        return\n",
    "    \n",
    "    print(f\"âš”ï¸  ê²€ìƒ‰ ë°©ë²• ë¹„êµ: '{question}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. ë²¡í„° ê²€ìƒ‰\n",
    "    print(\"\\nğŸ” 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "    print(\"-\" * 40)\n",
    "    vector_results = search_similar_chunks(question, limit=3, similarity_threshold=0.2)\n",
    "    \n",
    "    if vector_results:\n",
    "        for i, result in enumerate(vector_results, 1):\n",
    "            print(f\"{i}. {result['filename']} (ìœ ì‚¬ë„: {result['similarity']:.3f})\")\n",
    "            print(f\"   ğŸ“ {result['chunk_text'][:100]}...\")\n",
    "    else:\n",
    "        print(\"  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "    \n",
    "    # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ (PostgreSQL ì „ë¬¸ê²€ìƒ‰)\n",
    "    print(\"\\nğŸ”¤ 2. í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²€ìƒ‰\n",
    "            search_terms = question.split()\n",
    "            keyword_query = ' | '.join(search_terms[:3])  # ì²˜ìŒ 3ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©\n",
    "            \n",
    "            cur.execute(\"\"\"\n",
    "                SELECT filename, chunk_text, \n",
    "                       ts_rank(to_tsvector('english', chunk_text), \n",
    "                               plainto_tsquery('english', %s)) as rank\n",
    "                FROM documents\n",
    "                WHERE to_tsvector('english', chunk_text) @@ plainto_tsquery('english', %s)\n",
    "                ORDER BY rank DESC\n",
    "                LIMIT 3\n",
    "            \"\"\", (question, question))\n",
    "            \n",
    "            keyword_results = cur.fetchall()\n",
    "            \n",
    "            if keyword_results:\n",
    "                for i, result in enumerate(keyword_results, 1):\n",
    "                    print(f\"{i}. {result[0]} (ë­í¬: {result[2]:.4f})\")\n",
    "                    print(f\"   ğŸ“ {result[1][:100]}...\")\n",
    "            else:\n",
    "                print(\"  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "def analyze_embeddings():\n",
    "    \"\"\"\n",
    "    ì €ì¥ëœ ì„ë² ë”© ë°ì´í„° ë¶„ì„\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            # ê¸°ë³¸ í†µê³„\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT \n",
    "                    filename,\n",
    "                    COUNT(*) as chunk_count,\n",
    "                    AVG(LENGTH(chunk_text)) as avg_chunk_length,\n",
    "                    MAX(LENGTH(chunk_text)) as max_chunk_length,\n",
    "                    MIN(LENGTH(chunk_text)) as min_chunk_length\n",
    "                FROM documents\n",
    "                GROUP BY filename\n",
    "                ORDER BY chunk_count DESC\n",
    "            \"\"\")\n",
    "            \n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            print(\"ğŸ“Š ì„ë² ë”© ë°ì´í„° ë¶„ì„\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            df_stats = pd.DataFrame(results, columns=[\n",
    "                'íŒŒì¼ëª…', 'ì²­í¬ìˆ˜', 'í‰ê· ê¸¸ì´', 'ìµœëŒ€ê¸¸ì´', 'ìµœì†Œê¸¸ì´'\n",
    "            ])\n",
    "            \n",
    "            # ìˆ˜ì¹˜ ì»¬ëŸ¼ ë°˜ì˜¬ë¦¼\n",
    "            df_stats['í‰ê· ê¸¸ì´'] = df_stats['í‰ê· ê¸¸ì´'].round(0).astype(int)\n",
    "            \n",
    "            display(df_stats)\n",
    "            \n",
    "            # ì „ì²´ ìš”ì•½\n",
    "            total_chunks = df_stats['ì²­í¬ìˆ˜'].sum()\n",
    "            avg_chunk_size = df_stats['í‰ê· ê¸¸ì´'].mean()\n",
    "            \n",
    "            print(f\"\\nğŸ“‹ ì „ì²´ ìš”ì•½:\")\n",
    "            print(f\"  â€¢ ì´ ì²­í¬ ìˆ˜: {total_chunks:,}ê°œ\")\n",
    "            print(f\"  â€¢ í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_size:.0f}ì\")\n",
    "            print(f\"  â€¢ ì„ë² ë”© ì°¨ì›: {EMBEDDING_DIMENSION}\")\n",
    "            print(f\"  â€¢ ì‚¬ìš© ëª¨ë¸: {EMBEDDING_MODEL}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(\"ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ ì •ì˜ ì™„ë£Œ ğŸ“ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰\n",
    "if vector_db.conn:\n",
    "    print(\"ğŸ“ˆ ì„ë² ë”© ë°ì´í„° ë¶„ì„\")\n",
    "    analyze_embeddings()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âš”ï¸  ê²€ìƒ‰ ë°©ë²• ë¹„êµ í…ŒìŠ¤íŠ¸\")\n",
    "    compare_search_methods(\"íŒŒì¼ ì—…ë¡œë“œ API ì‚¬ìš©ë²•\")\n",
    "else:\n",
    "    print(\"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ì •ë¦¬ ë° ë¦¬ì†ŒìŠ¤ í•´ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ (í•„ìš”ì‹œ)\n",
    "# vector_db.close()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ‰ PostgreSQL + pgvector ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ì™„ë£Œ!\n",
    "\n",
    "âœ… ì£¼ìš” ê¸°ëŠ¥:\n",
    "  â€¢ ë¬¸ì„œ ìë™ ì²­í‚¹ ë° ì„ë² ë”© ìƒì„±\n",
    "  â€¢ PostgreSQL + pgvector ë²¡í„° ì €ì¥\n",
    "  â€¢ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰\n",
    "  â€¢ Ollama ëª¨ë¸ê³¼ ì—°ë™ëœ ì§ˆì˜ì‘ë‹µ\n",
    "  â€¢ ì„±ëŠ¥ ë¶„ì„ ë° ë¹„êµ ë„êµ¬\n",
    "\n",
    "ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤:\n",
    "  â€¢ vector_qa(question) - ë²¡í„° ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ\n",
    "  â€¢ search_similar_chunks(query) - ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰\n",
    "  â€¢ show_similar_chunks(question) - ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ\n",
    "  â€¢ interactive_vector_qa() - ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤\n",
    "  â€¢ compare_search_methods(question) - ê²€ìƒ‰ ë°©ë²• ë¹„êµ\n",
    "  â€¢ analyze_embeddings() - ì„ë² ë”© ë°ì´í„° ë¶„ì„\n",
    "\n",
    "ğŸ’¡ ì¥ì :\n",
    "  â€¢ ë” ì •í™•í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰\n",
    "  â€¢ ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "  â€¢ í™•ì¥ ê°€ëŠ¥í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤\n",
    "  â€¢ ì‹¤ì‹œê°„ ê²€ìƒ‰ ì„±ëŠ¥\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
