{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + PostgreSQL Vector Database 질의응답 시스템\n",
    "\n",
    "이 노트북은 PostgreSQL + pgvector를 사용하여 문서 임베딩을 저장하고,  \n",
    "벡터 유사도 검색을 통해 더 정확한 질의응답을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Vector Database\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "# Ollama\n",
    "import ollama\n",
    "\n",
    "print(\"라이브러리 로딩 완료 ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터베이스 설정 및 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스 설정\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'vector_qa',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres'\n",
    "}\n",
    "\n",
    "# 임베딩 설정\n",
    "EMBEDDING_MODEL = 'nomic-embed-text'  # Ollama 임베딩 모델\n",
    "EMBEDDING_DIMENSION = 768  # 임베딩 차원수\n",
    "CHUNK_SIZE = 1000  # 텍스트 청크 크기\n",
    "\n",
    "print(\"설정 완료 ⚙️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDatabase:\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.conn = None\n",
    "        self.embedding_model = EMBEDDING_MODEL\n",
    "        self.embedding_dim = EMBEDDING_DIMENSION\n",
    "    \n",
    "    def connect(self) -> bool:\n",
    "        \"\"\"데이터베이스에 연결\"\"\"\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(**self.config)\n",
    "            self.conn.autocommit = True\n",
    "            register_vector(self.conn)\n",
    "            print(\"✅ PostgreSQL 연결 성공\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ PostgreSQL 연결 실패: {e}\")\n",
    "            print(\"\\n💡 PostgreSQL 서버가 실행 중인지 확인하세요:\")\n",
    "            print(\"   brew services start postgresql@14\")\n",
    "            print(\"   또는 Docker: docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:14\")\n",
    "            return False\n",
    "    \n",
    "    def create_database_if_not_exists(self):\n",
    "        \"\"\"데이터베이스가 없으면 생성\"\"\"\n",
    "        try:\n",
    "            # postgres 데이터베이스에 연결하여 vector_qa 데이터베이스 생성\n",
    "            temp_config = self.config.copy()\n",
    "            temp_config['database'] = 'postgres'\n",
    "            \n",
    "            temp_conn = psycopg2.connect(**temp_config)\n",
    "            temp_conn.autocommit = True\n",
    "            \n",
    "            with temp_conn.cursor() as cur:\n",
    "                # 데이터베이스 존재 확인\n",
    "                cur.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (self.config['database'],))\n",
    "                if not cur.fetchone():\n",
    "                    cur.execute(f\"CREATE DATABASE {self.config['database']}\")\n",
    "                    print(f\"✅ 데이터베이스 '{self.config['database']}' 생성 완료\")\n",
    "                else:\n",
    "                    print(f\"ℹ️  데이터베이스 '{self.config['database']}' 이미 존재\")\n",
    "            \n",
    "            temp_conn.close()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 데이터베이스 생성 실패: {e}\")\n",
    "    \n",
    "    def setup_tables(self):\n",
    "        \"\"\"필요한 테이블 및 확장 설정\"\"\"\n",
    "        if not self.conn:\n",
    "            print(\"❌ 데이터베이스 연결이 필요합니다\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                # pgvector 확장 설치\n",
    "                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "                print(\"✅ pgvector 확장 설치 완료\")\n",
    "                \n",
    "                # 문서 테이블 생성\n",
    "                cur.execute(f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS documents (\n",
    "                        id SERIAL PRIMARY KEY,\n",
    "                        filename VARCHAR(255) NOT NULL,\n",
    "                        content TEXT NOT NULL,\n",
    "                        chunk_index INTEGER NOT NULL,\n",
    "                        chunk_text TEXT NOT NULL,\n",
    "                        embedding vector({self.embedding_dim}),\n",
    "                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                        metadata JSONB\n",
    "                    )\n",
    "                \"\"\")\n",
    "                \n",
    "                # 벡터 유사도 검색을 위한 인덱스 생성\n",
    "                cur.execute(\"\"\"\n",
    "                    CREATE INDEX IF NOT EXISTS documents_embedding_idx \n",
    "                    ON documents USING ivfflat (embedding vector_cosine_ops) \n",
    "                    WITH (lists = 100)\n",
    "                \"\"\")\n",
    "                \n",
    "                print(\"✅ 테이블 및 인덱스 설정 완료\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 테이블 설정 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"데이터베이스 연결 종료\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"🔒 데이터베이스 연결 종료\")\n",
    "\n",
    "# 벡터 데이터베이스 초기화\n",
    "vector_db = VectorDatabase(DB_CONFIG)\n",
    "print(\"VectorDatabase 클래스 정의 완료 🗄️\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터베이스 초기화 및 연결 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스 생성 (필요한 경우)\n",
    "vector_db.create_database_if_not_exists()\n",
    "\n",
    "# 데이터베이스 연결\n",
    "if vector_db.connect():\n",
    "    # 테이블 설정\n",
    "    vector_db.setup_tables()\n",
    "else:\n",
    "    print(\"⚠️  데이터베이스 연결에 실패했습니다. PostgreSQL 설정을 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 임베딩 생성 및 문서 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str, model: str = EMBEDDING_MODEL) -> List[float]:\n",
    "    \"\"\"\n",
    "    Ollama를 사용하여 텍스트 임베딩 생성\n",
    "    \n",
    "    Args:\n",
    "        text (str): 임베딩할 텍스트\n",
    "        model (str): 사용할 임베딩 모델\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: 임베딩 벡터\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = ollama.embeddings(model=model, prompt=text)\n",
    "        return response['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 임베딩 생성 실패: {e}\")\n",
    "        print(f\"💡 '{model}' 모델이 설치되어 있는지 확인하세요: ollama pull {model}\")\n",
    "        return None\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    긴 텍스트를 청크로 분할\n",
    "    \n",
    "    Args:\n",
    "        text (str): 분할할 텍스트\n",
    "        chunk_size (int): 청크 크기\n",
    "        overlap (int): 청크 간 겹치는 부분\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: 텍스트 청크 리스트\n",
    "    \"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        if end >= len(text):\n",
    "            chunks.append(text[start:])\n",
    "            break\n",
    "        \n",
    "        # 단어 경계에서 자르기\n",
    "        while end > start and text[end] not in ' \\n\\t.!?;':\n",
    "            end -= 1\n",
    "        \n",
    "        if end == start:\n",
    "            end = start + chunk_size\n",
    "        \n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def load_and_embed_documents(doc_folder: str = 'doc') -> bool:\n",
    "    \"\"\"\n",
    "    문서를 로딩하고 임베딩을 생성하여 데이터베이스에 저장\n",
    "    \n",
    "    Args:\n",
    "        doc_folder (str): 문서 폴더 경로\n",
    "    \n",
    "    Returns:\n",
    "        bool: 성공 여부\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"❌ 데이터베이스 연결이 필요합니다\")\n",
    "        return False\n",
    "    \n",
    "    doc_path = Path(doc_folder)\n",
    "    if not doc_path.exists():\n",
    "        print(f\"❌ '{doc_folder}' 폴더가 존재하지 않습니다\")\n",
    "        return False\n",
    "    \n",
    "    md_files = list(doc_path.glob('*.md'))\n",
    "    if not md_files:\n",
    "        print(f\"❌ '{doc_folder}' 폴더에 마크다운 파일이 없습니다\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"📚 {len(md_files)}개 문서 처리 시작...\")\n",
    "    \n",
    "    try:\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            # 기존 데이터 삭제 (선택사항)\n",
    "            # cur.execute(\"DELETE FROM documents\")\n",
    "            \n",
    "            total_chunks = 0\n",
    "            \n",
    "            for md_file in md_files:\n",
    "                print(f\"\\n📄 처리 중: {md_file.name}\")\n",
    "                \n",
    "                try:\n",
    "                    with open(md_file, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # 텍스트 청킹\n",
    "                    chunks = chunk_text(content)\n",
    "                    print(f\"  📝 {len(chunks)}개 청크로 분할\")\n",
    "                    \n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        if len(chunk.strip()) < 50:  # 너무 짧은 청크는 스킵\n",
    "                            continue\n",
    "                        \n",
    "                        # 임베딩 생성\n",
    "                        embedding = generate_embedding(chunk)\n",
    "                        if embedding is None:\n",
    "                            continue\n",
    "                        \n",
    "                        # 데이터베이스에 저장\n",
    "                        metadata = {\n",
    "                            'file_size': len(content),\n",
    "                            'chunk_size': len(chunk)\n",
    "                        }\n",
    "                        \n",
    "                        cur.execute(\"\"\"\n",
    "                            INSERT INTO documents \n",
    "                            (filename, content, chunk_index, chunk_text, embedding, metadata)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                        \"\"\", (\n",
    "                            md_file.name,\n",
    "                            content,\n",
    "                            i,\n",
    "                            chunk,\n",
    "                            embedding,\n",
    "                            json.dumps(metadata)\n",
    "                        ))\n",
    "                        \n",
    "                        total_chunks += 1\n",
    "                        \n",
    "                        if (i + 1) % 5 == 0:\n",
    "                            print(f\"    ⚡ {i + 1}개 청크 처리 완료\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"  ❌ {md_file.name} 처리 실패: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"\\n✅ 총 {total_chunks}개 청크를 데이터베이스에 저장했습니다\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 문서 임베딩 처리 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"문서 처리 함수 정의 완료 📚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문서 로딩 및 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 다운로드 확인\n",
    "try:\n",
    "    test_embedding = generate_embedding(\"테스트 텍스트\")\n",
    "    if test_embedding:\n",
    "        print(f\"✅ 임베딩 모델 '{EMBEDDING_MODEL}' 사용 가능\")\n",
    "        print(f\"📏 임베딩 차원: {len(test_embedding)}\")\n",
    "        \n",
    "        # 임베딩 차원 업데이트 (실제 모델의 차원과 맞춤)\n",
    "        if len(test_embedding) != EMBEDDING_DIMENSION:\n",
    "            EMBEDDING_DIMENSION = len(test_embedding)\n",
    "            vector_db.embedding_dim = EMBEDDING_DIMENSION\n",
    "            print(f\"🔧 임베딩 차원을 {EMBEDDING_DIMENSION}으로 업데이트\")\n",
    "    else:\n",
    "        print(f\"❌ 임베딩 모델을 다운로드하세요: ollama pull {EMBEDDING_MODEL}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 임베딩 테스트 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 로딩 및 임베딩 생성 실행\n",
    "if vector_db.conn:\n",
    "    success = load_and_embed_documents('doc')\n",
    "    if success:\n",
    "        # 저장된 문서 통계 확인\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT COUNT(*) FROM documents\")\n",
    "            total_chunks = cur.fetchone()[0]\n",
    "            \n",
    "            cur.execute(\"SELECT COUNT(DISTINCT filename) FROM documents\")\n",
    "            total_files = cur.fetchone()[0]\n",
    "            \n",
    "            print(f\"\\n📊 데이터베이스 현황:\")\n",
    "            print(f\"  📁 파일 수: {total_files}개\")\n",
    "            print(f\"  📄 청크 수: {total_chunks}개\")\n",
    "else:\n",
    "    print(\"⚠️  데이터베이스 연결이 필요합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 벡터 유사도 검색 및 질의응답 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query: str, limit: int = 5, similarity_threshold: float = 0.7) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    벡터 유사도를 사용하여 관련 문서 청크 검색\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 쿼리\n",
    "        limit (int): 반환할 최대 결과 수\n",
    "        similarity_threshold (float): 유사도 임계값 (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: 유사한 문서 청크 리스트\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"❌ 데이터베이스 연결이 필요합니다\")\n",
    "        return []\n",
    "    \n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = generate_embedding(query)\n",
    "    if not query_embedding:\n",
    "        print(\"❌ 쿼리 임베딩 생성 실패\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            # 코사인 유사도 검색\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT \n",
    "                    id,\n",
    "                    filename,\n",
    "                    chunk_index,\n",
    "                    chunk_text,\n",
    "                    metadata,\n",
    "                    1 - (embedding <=> %s) AS similarity\n",
    "                FROM documents\n",
    "                WHERE 1 - (embedding <=> %s) > %s\n",
    "                ORDER BY embedding <=> %s\n",
    "                LIMIT %s\n",
    "            \"\"\", (query_embedding, query_embedding, similarity_threshold, query_embedding, limit))\n",
    "            \n",
    "            results = []\n",
    "            for row in cur.fetchall():\n",
    "                results.append({\n",
    "                    'id': row[0],\n",
    "                    'filename': row[1],\n",
    "                    'chunk_index': row[2],\n",
    "                    'chunk_text': row[3],\n",
    "                    'metadata': json.loads(row[4]) if row[4] else {},\n",
    "                    'similarity': float(row[5])\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 벡터 검색 실패: {e}\")\n",
    "        return []\n",
    "\n",
    "def vector_qa(question: str, model_name: str = 'llama3.1', max_context_length: int = 4000) -> str:\n",
    "    \"\"\"\n",
    "    벡터 검색 기반 질의응답\n",
    "    \n",
    "    Args:\n",
    "        question (str): 질문\n",
    "        model_name (str): 사용할 Ollama 모델\n",
    "        max_context_length (int): 최대 컨텍스트 길이\n",
    "    \n",
    "    Returns:\n",
    "        str: 답변\n",
    "    \"\"\"\n",
    "    print(f\"🔍 질문 분석 중: {question}\")\n",
    "    \n",
    "    # 관련 문서 청크 검색\n",
    "    similar_chunks = search_similar_chunks(question, limit=10, similarity_threshold=0.3)\n",
    "    \n",
    "    if not similar_chunks:\n",
    "        return \"❌ 관련된 문서를 찾을 수 없습니다. 다른 질문을 시도해보세요.\"\n",
    "    \n",
    "    print(f\"📚 {len(similar_chunks)}개 관련 문서 청크 발견\")\n",
    "    \n",
    "    # 컨텍스트 구성 (길이 제한)\n",
    "    context_parts = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for chunk in similar_chunks:\n",
    "        chunk_text = f\"=== {chunk['filename']} (유사도: {chunk['similarity']:.3f}) ===\\n{chunk['chunk_text']}\\n\"\n",
    "        \n",
    "        if current_length + len(chunk_text) > max_context_length:\n",
    "            break\n",
    "        \n",
    "        context_parts.append(chunk_text)\n",
    "        current_length += len(chunk_text)\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 프롬프트 구성\n",
    "    prompt = f\"\"\"다음 문서들을 기반으로 질문에 정확하고 자세하게 답변해주세요:\n",
    "\n",
    "관련 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변 규칙:\n",
    "1. 문서의 내용만을 기반으로 답변하세요\n",
    "2. 문서에 없는 내용은 추측하지 마세요\n",
    "3. 가능하면 구체적인 예시나 코드를 포함하세요\n",
    "4. 답변의 근거가 되는 문서명을 언급하세요\n",
    "\n",
    "답변:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"🤖 {model_name} 모델로 답변 생성 중...\")\n",
    "        \n",
    "        response = ollama.chat(model=model_name, messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        answer = response['message']['content']\n",
    "        \n",
    "        # 참고 문서 정보 추가\n",
    "        referenced_files = set(chunk['filename'] for chunk in similar_chunks[:5])\n",
    "        references = f\"\\n\\n📋 **참고 문서:** {', '.join(referenced_files)}\"\n",
    "        \n",
    "        return answer + references\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ 답변 생성 실패: {e}\"\n",
    "\n",
    "def show_similar_chunks(question: str, limit: int = 5):\n",
    "    \"\"\"\n",
    "    질문과 유사한 문서 청크들을 표시\n",
    "    \n",
    "    Args:\n",
    "        question (str): 검색 질문\n",
    "        limit (int): 표시할 결과 수\n",
    "    \"\"\"\n",
    "    chunks = search_similar_chunks(question, limit=limit, similarity_threshold=0.2)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"❌ 유사한 문서를 찾을 수 없습니다\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🔍 '{question}'과 유사한 문서 청크들:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n{i}. 📄 {chunk['filename']} (청크 {chunk['chunk_index']})\")\n",
    "        print(f\"   🎯 유사도: {chunk['similarity']:.3f}\")\n",
    "        print(f\"   📝 내용: {chunk['chunk_text'][:200]}{'...' if len(chunk['chunk_text']) > 200 else ''}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "print(\"벡터 검색 및 QA 함수 정의 완료 🔍\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 벡터 검색 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 검색 테스트\n",
    "if vector_db.conn:\n",
    "    test_query = \"Network API 사용법\"\n",
    "    print(f\"🧪 테스트 쿼리: '{test_query}'\")\n",
    "    \n",
    "    # 유사 문서 청크 표시\n",
    "    show_similar_chunks(test_query, limit=3)\n",
    "else:\n",
    "    print(\"⚠️  데이터베이스 연결이 필요합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 벡터 기반 질의응답 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 질의응답\n",
    "sample_questions = [\n",
    "    \"파일 업로드는 어떻게 하나요?\",\n",
    "    \"카메라 촬영 기능을 사용하는 방법은?\",\n",
    "    \"JWT 토큰 관리는 어떻게 해야 하나요?\"\n",
    "]\n",
    "\n",
    "if vector_db.conn:\n",
    "    print(\"🤖 벡터 기반 질의응답 예시\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, question in enumerate(sample_questions, 1):\n",
    "        print(f\"\\n{i}. ❓ 질문: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        answer = vector_qa(question)\n",
    "        print(f\"💬 답변:\\n{answer}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if i < len(sample_questions):\n",
    "            input(\"\\n⏳ 다음 질문으로 넘어가려면 Enter를 누르세요...\")\n",
    "else:\n",
    "    print(\"⚠️  데이터베이스 연결이 필요합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 대화형 벡터 질의응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_vector_qa():\n",
    "    \"\"\"\n",
    "    대화형 벡터 기반 질의응답 인터페이스\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"❌ 데이터베이스 연결이 필요합니다\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"🚀 벡터 데이터베이스 기반 질의응답 시스템\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"특징:\")\n",
    "    print(\"  🔍 벡터 유사도 검색으로 정확한 문서 검색\")\n",
    "    print(\"  📚 PostgreSQL + pgvector 활용\")\n",
    "    print(\"  🎯 문서 청킹 및 임베딩 기반 검색\")\n",
    "    print()\n",
    "    print(\"사용법:\")\n",
    "    print(\"  - 질문을 입력하면 관련 문서를 검색하여 답변합니다\")\n",
    "    print(\"  - 'quit' 또는 'exit'로 종료\")\n",
    "    print(\"  - '!search [질문]'으로 유사 문서만 검색\")\n",
    "    print(\"  - '!stats'로 데이터베이스 통계 확인\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n🤖 질문을 입력하세요: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', '종료']:\n",
    "            print(\"👋 벡터 질의응답을 종료합니다.\")\n",
    "            break\n",
    "        \n",
    "        if question.startswith('!search '):\n",
    "            search_query = question[8:].strip()\n",
    "            show_similar_chunks(search_query, limit=5)\n",
    "            continue\n",
    "        \n",
    "        if question == '!stats':\n",
    "            try:\n",
    "                with vector_db.conn.cursor() as cur:\n",
    "                    cur.execute(\"SELECT COUNT(*) FROM documents\")\n",
    "                    total_chunks = cur.fetchone()[0]\n",
    "                    \n",
    "                    cur.execute(\"SELECT COUNT(DISTINCT filename) FROM documents\")\n",
    "                    total_files = cur.fetchone()[0]\n",
    "                    \n",
    "                    cur.execute(\"SELECT filename, COUNT(*) FROM documents GROUP BY filename\")\n",
    "                    file_stats = cur.fetchall()\n",
    "                    \n",
    "                    print(f\"\\n📊 데이터베이스 통계:\")\n",
    "                    print(f\"  📁 총 파일 수: {total_files}개\")\n",
    "                    print(f\"  📄 총 청크 수: {total_chunks}개\")\n",
    "                    print(f\"  📋 파일별 청크 수:\")\n",
    "                    for filename, count in file_stats:\n",
    "                        print(f\"    - {filename}: {count}개\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 통계 조회 실패: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if not question:\n",
    "            print(\"질문을 입력해주세요.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        answer = vector_qa(question)\n",
    "        print(\"\\n💬 답변:\")\n",
    "        print(answer)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "# 대화형 인터페이스 시작 (필요시 실행)\n",
    "# interactive_vector_qa()\n",
    "\n",
    "print(\"대화형 벡터 QA 함수 정의 완료 💬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 커스텀 벡터 질의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 질문 입력하여 테스트\n",
    "custom_question = \"bizMOB에서 데이터베이스 연결하는 방법을 알려주세요\"\n",
    "\n",
    "if custom_question.strip() and vector_db.conn:\n",
    "    print(f\"❓ 사용자 질문: {custom_question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 벡터 기반 답변 생성\n",
    "    answer = vector_qa(custom_question)\n",
    "    print(f\"\\n💬 벡터 검색 기반 답변:\")\n",
    "    display(Markdown(answer))\n",
    "    \n",
    "    print(\"\\n🔍 관련 문서 청크 분석:\")\n",
    "    show_similar_chunks(custom_question, limit=3)\n",
    "elif not custom_question.strip():\n",
    "    print(\"위의 custom_question 변수에 질문을 입력하고 셀을 실행하세요.\")\n",
    "else:\n",
    "    print(\"⚠️  데이터베이스 연결이 필요합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 성능 비교 및 분석 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search_methods(question: str):\n",
    "    \"\"\"\n",
    "    벡터 검색과 기존 텍스트 검색 비교\n",
    "    \n",
    "    Args:\n",
    "        question (str): 비교할 질문\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"❌ 데이터베이스 연결이 필요합니다\")\n",
    "        return\n",
    "    \n",
    "    print(f\"⚔️  검색 방법 비교: '{question}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. 벡터 검색\n",
    "    print(\"\\n🔍 1. 벡터 유사도 검색 결과:\")\n",
    "    print(\"-\" * 40)\n",
    "    vector_results = search_similar_chunks(question, limit=3, similarity_threshold=0.2)\n",
    "    \n",
    "    if vector_results:\n",
    "        for i, result in enumerate(vector_results, 1):\n",
    "            print(f\"{i}. {result['filename']} (유사도: {result['similarity']:.3f})\")\n",
    "            print(f\"   📝 {result['chunk_text'][:100]}...\")\n",
    "    else:\n",
    "        print(\"  검색 결과 없음\")\n",
    "    \n",
    "    # 2. 키워드 검색 (PostgreSQL 전문검색)\n",
    "    print(\"\\n🔤 2. 키워드 검색 결과:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            # 간단한 키워드 검색\n",
    "            search_terms = question.split()\n",
    "            keyword_query = ' | '.join(search_terms[:3])  # 처음 3개 단어만 사용\n",
    "            \n",
    "            cur.execute(\"\"\"\n",
    "                SELECT filename, chunk_text, \n",
    "                       ts_rank(to_tsvector('english', chunk_text), \n",
    "                               plainto_tsquery('english', %s)) as rank\n",
    "                FROM documents\n",
    "                WHERE to_tsvector('english', chunk_text) @@ plainto_tsquery('english', %s)\n",
    "                ORDER BY rank DESC\n",
    "                LIMIT 3\n",
    "            \"\"\", (question, question))\n",
    "            \n",
    "            keyword_results = cur.fetchall()\n",
    "            \n",
    "            if keyword_results:\n",
    "                for i, result in enumerate(keyword_results, 1):\n",
    "                    print(f\"{i}. {result[0]} (랭크: {result[2]:.4f})\")\n",
    "                    print(f\"   📝 {result[1][:100]}...\")\n",
    "            else:\n",
    "                print(\"  검색 결과 없음\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  키워드 검색 오류: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "def analyze_embeddings():\n",
    "    \"\"\"\n",
    "    저장된 임베딩 데이터 분석\n",
    "    \"\"\"\n",
    "    if not vector_db.conn:\n",
    "        print(\"❌ 데이터베이스 연결이 필요합니다\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with vector_db.conn.cursor() as cur:\n",
    "            # 기본 통계\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT \n",
    "                    filename,\n",
    "                    COUNT(*) as chunk_count,\n",
    "                    AVG(LENGTH(chunk_text)) as avg_chunk_length,\n",
    "                    MAX(LENGTH(chunk_text)) as max_chunk_length,\n",
    "                    MIN(LENGTH(chunk_text)) as min_chunk_length\n",
    "                FROM documents\n",
    "                GROUP BY filename\n",
    "                ORDER BY chunk_count DESC\n",
    "            \"\"\")\n",
    "            \n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            print(\"📊 임베딩 데이터 분석\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            df_stats = pd.DataFrame(results, columns=[\n",
    "                '파일명', '청크수', '평균길이', '최대길이', '최소길이'\n",
    "            ])\n",
    "            \n",
    "            # 수치 컬럼 반올림\n",
    "            df_stats['평균길이'] = df_stats['평균길이'].round(0).astype(int)\n",
    "            \n",
    "            display(df_stats)\n",
    "            \n",
    "            # 전체 요약\n",
    "            total_chunks = df_stats['청크수'].sum()\n",
    "            avg_chunk_size = df_stats['평균길이'].mean()\n",
    "            \n",
    "            print(f\"\\n📋 전체 요약:\")\n",
    "            print(f\"  • 총 청크 수: {total_chunks:,}개\")\n",
    "            print(f\"  • 평균 청크 크기: {avg_chunk_size:.0f}자\")\n",
    "            print(f\"  • 임베딩 차원: {EMBEDDING_DIMENSION}\")\n",
    "            print(f\"  • 사용 모델: {EMBEDDING_MODEL}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 분석 실패: {e}\")\n",
    "\n",
    "print(\"성능 분석 도구 정의 완료 📈\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 분석 실행\n",
    "if vector_db.conn:\n",
    "    print(\"📈 임베딩 데이터 분석\")\n",
    "    analyze_embeddings()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"⚔️  검색 방법 비교 테스트\")\n",
    "    compare_search_methods(\"파일 업로드 API 사용법\")\n",
    "else:\n",
    "    print(\"⚠️  데이터베이스 연결이 필요합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 정리 및 리소스 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스 연결 종료 (필요시)\n",
    "# vector_db.close()\n",
    "\n",
    "print(\"\"\"\n",
    "🎉 PostgreSQL + pgvector 기반 질의응답 시스템 완료!\n",
    "\n",
    "✅ 주요 기능:\n",
    "  • 문서 자동 청킹 및 임베딩 생성\n",
    "  • PostgreSQL + pgvector 벡터 저장\n",
    "  • 코사인 유사도 기반 검색\n",
    "  • Ollama 모델과 연동된 질의응답\n",
    "  • 성능 분석 및 비교 도구\n",
    "\n",
    "🔧 사용 가능한 함수들:\n",
    "  • vector_qa(question) - 벡터 기반 질의응답\n",
    "  • search_similar_chunks(query) - 유사 문서 검색\n",
    "  • show_similar_chunks(question) - 검색 결과 표시\n",
    "  • interactive_vector_qa() - 대화형 인터페이스\n",
    "  • compare_search_methods(question) - 검색 방법 비교\n",
    "  • analyze_embeddings() - 임베딩 데이터 분석\n",
    "\n",
    "💡 장점:\n",
    "  • 더 정확한 의미 기반 검색\n",
    "  • 대용량 문서 처리 가능\n",
    "  • 확장 가능한 벡터 데이터베이스\n",
    "  • 실시간 검색 성능\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
